Igazat mond az NVIDIA a CUDA-ról?

[HWSW] 2008 végének egyik érdekessége, hogy immár a földönkívüli intelligencia után kutató SETI@home projekt is képest kiaknázni a grafikus processzorokban rejlõ potenciált. A NVIDIA GeForce 8-sorozatú vagy újabb videokártyák immár képesek átvenni a processzortól a rádiójelek elemzését, aminek persze csak akkor van értelme, amennyiben a grafikus kártyánk potensebb számítási erõforrásokkal rendelkezik processzorunknál. Az NIVIDA szerint drasztikus a gyorsulás, egyes felhasználók szerint viszont a cég egyszerûen hazudik, mikor a CUDA és a GeForce-ok egyértelmû fölényérõl beszél.  
Cudar állítások
Amennyiben a BOINC (Berkeley Open Infrastructure for Network Computing) keretrendszer 6.4.4 verziójával, valamint megfelelõ NVIDIA grafikus chippel és driverrel rendelkezünk, a CUDA-verziójú (az NVIDIA nem grafikus számításokra létrehozott API-ja) SETI@home klienst a BOINC automatikusan letölti a gépre. Ezzel a központi processzorról átkerül a számítás a grafikus magra, ami ideális esetben a feldolgozás gyorsulását jelenti a tipikusan játékra vagy professzionális grafikus munkára használt PC-ken.
Az NVIDIA GeForce GTX 280 chip a vállalat szerint közel kétszer nagyobb teljesítményre képes SETI@home feldolgozásban a jelenleg kapható legerõsebb x86-os processzornál, a 3,2 gigahertzes Nehelamnél (Core i7 965), miközben harmadannyiba kerül -- igaz, a fogyasztása viszont másfélszerese. A cég kommünikéje szerint a gyorsulás egy "tipikus" kétmagos rendszerhez képest tízszeres is lehet, míg egy 2,66 gigahertzes Core 2 Duo E8200-hez viszonyítva a GTX 280 hat és félszer, a 9600 GT pedig két és félszer több csomagot dolgoz fel naponta.
Teljesítmény SETI@home alatt, az NVIDIA szerint; Forrás: NVIDIA
Jelenleg nem lehetséges, hogy azonos projekten egyszerre dolgozzon a processzor és a grafikus chip, de a processzor erõforrásait hozzá lehet rendelni más BOINC-projektekhez, mint például a forgó neutroncsillagok után kutató Einstein@home, a fehérjék térbeli felépítésének vizsgálatát folytató Rosetta@home, idõjárási modellek kifejlesztése vagy a SZTAKI matematikai problémáinak megoldása. A processzormagok és a grafikus lapka együttes meghajtása SETI@home alatt a jövõben lehetséges lesz, ígérik a fejlesztõk -- egyelõre azonban nem mûködnek egyszerre.
Az NIVIDA látványos teljesítménynövekedésrõl szóló állításait hevesen vitatják egyes felhasználók, többen állítják, nem tudják reprodukálni a cég eredményeit, és tapasztalataik alapján a grafikus chipek fölénye nem mutatkozik meg konzisztensen. Sokan azzal gyanúsítják a céget, hogy félre akarja vezetni a publikumot a mérések pontos hátterének elhallgatásával. Márpedig torzításra sajnos számos lehetõség nyílik egy ilyen összevetésben, például az x86-processzorokon futtatott kód optimalizáltsága, de az egyes csomagok (feldolgozási egységek) között is jelentõs szórás mutatkozik.
Egypontos statisztikával a marketing ellen
Egy felhasználó mérései alapján egy gyári GeForce 9600 GSO (96 stream processzor, 550 megahertz, 192 bites memóriabusz), 2,2-szer gyorsabban  (vagyis kevesbb, mint fele idõ alatt) dolgozta fel ugyanazt a csomagot, mint egy 2,66 gigahertzes Intel Core 2 Quad Q9450 (Yorkfield, 2x 6 MB L2 cache, 1333 MT/s FSB) egy magja. Tekintve, hogy a SETI@home egymástól független szálakként indít további feldolgozásokat többmagos rendszereken, a kód számításintenzív és jól cache-elhetõ a masszív L2 tárakban, továbbá kevéssé terheli a memóriaalrendszert, a lineárishoz közelítõ magas fokú skálázódás jellemzõ a párhuzamos feldolgozások számának növelésével. Mind a négy mag meghajtásával tehát már a négymagos x86-processzor kerülne ki gyõztesen, ennek pontos mértékét azonban nehéz megbecsülni, véleményük szerint 40-60 százalék közé esne tipikusan.
Ez élesen szemben áll az NVIDIA azon állításával, mely szerint egy 9600 GT két és félszereres teljesítményre képes egy Core 2 Duo E8200-zal szemben, hiszen a 9600 GT gyengébb is a GSO-nál számításintenzív feladatokban, mindössze 64 processzoregységgel rendelkezik, amit csak némileg ellensúlyozhat a 256 bites memóriainterfész. Végeredményben a 9600 GT bár gyorsabb lehet a kétmagos processzornál, a különbség közel sem lehet ekkora, a két chip nagyon közel teljesítene egymáshoz -- igaz, ezzel együtt is sokkal jobb ár-teljesítmény mutatóval rendelkezne a GeForce 9600 GT, mely úgy 40 százalékkal olcsóbb az E8200-nál, ami 15-18 ezer forintot takar, üzlettõl és kártyamodelltõl függõen.
Az NVIDIA állításai, valamint a felhasználók tapasztalati közötti szakadékra két tényezõ szolgáltathatja a legfõbb magyarázatot. Az NVIDIA valószínûleg generikus, optimalizálatlan  SETI@home klienst használt, miközben a közösségtõl hozzáférhetõek erõteljesen optimalizált verziók, melyek kihasználják például a modern x86 chipekben megtalálható  vektoros utasításkészleteket -- ezeket a keménymag, tehát akiket érdekelhet a CUDA, használják is. A fenti mérési eredmények például SSE4.1 optimalizációval születtek. A számítások vektorizálása nagyban megnöveli az adatszintû párhuzamosságot, ezzel pedig a feldolgozás sebességét gyorsítja drasztikusan. Az eltérés másik oka  az egyes adatcsomagok jellemzõi lehetnek, nem egyforma, hogy milyen mûveletbõl mennyi igényelnek.
Az E8200-zal hasonló árban lévõ 9800 GTX nagyjából valamivel kevesebb mint másfélszer nagyobb számítási teljesítményre képes SETI@home alatt, mint a 9600 GSO, vagyis már maga mögé utasítaná a kétmagos E8200-zat, de messze nem az NVIDIA által jelzett 276 százalékos fölénnyel. A GeForce 200-as sorozat egy újabb másfélszeres szorzót hoz magával számítási teljesítményben, a GTX 260 nem sokkal, negyedével magasabb áron, vagyis kevesebb pénzért nyújt magasabb teljesítményt a Q9450-nél. A Nehalem-alapú Core i7 920 azonban ismét az x86 javára billenti a teljesítmény- és ár-értékbeli mutatókat, nem beszélve az energiahatékonyságról: a GT260 kártyák akár 200 wattot is elemésztenek, míg a Nehalem 130 wattnál kevesebbel is beéri, ami non-stop üzem esetén nem elhanyagolható éves szinten.

A pontos képhez természetesen rengeteg, jól dokumentált mérési pontra volna szükség, melyek egyelõre nem állnak rendelkezésre,  de hogy a GeForce és CUDA elsöprõ teljesítménybeli fölényt vívna ki SETI@home alatt, az a felhasználók tapasztalatai és mérései alapján minimum kétségesnek, de leginkább valótlannak tûnik. Ez a történet mindenesetre felveti a kérdést, hogy az NVIDIA többi CUDA sikertörténete mennyiben fedi a valóságot, így például valóban gyorsabb-e a videofeldolgozás GeForce kártyákon, mint megfelelõ enkóderrel egy kurrens négymagos processzoron.
A vallásháború folytatódik
Az NVIDIA által erõltetett "világok harca" helyett a felhasználók számára az volna a legértékesebb, ha a teljesítményéhes szoftverek a processzor és a grafikus chipek erõforrásait is ki tudnák egyszerre használni, hasonlóan a játékokhoz vagy a 3D-gyorsított CAD/CAM alkalmazásokhoz. Fejlesztõinek ígérete szerint ebben az irányban tart a SETI@home is, és rövidesen párhuzamosan, kéz a kézben folytathatják a földönkívüliek keresését az x86-os és grafikus processzorok -- remélhetõleg nem lesz ez másként sok más alkalmazásnál sem, és akár megduplázhatjuk gépünk teljesítményét egyes alkalmazások alatt anélkül, hogy költséges hardverfejlesztésekbe kellene verni magunkat.
Naivitás volna azonban azt gondolni, hogy a vallásháború ne folytatódna még jó ideig, ahogyan a felek emelik a téteket. Kétségtelen, hogy az NVIDIA G80 és GT200 architektúrái, valamint a CUDA megmutatták, valóban milyen messze jutottak programozhatóságban a grafikus chipek, és egyes területeken milyen hatalmas potenciállal bírnak. Éppen ez váltotta ki az Intel Larrabee projektjét, mely egy masszívan párhuzamos felépítésû x86-os grafikus processzor több tucat vektorizált maggal, hogy felvegye a kesztyût az egyre rugalmasabban programozható, erõteljes grafikus architektúrákkal szemben.
[ ] Világok harca: felbukkant az elsõ GPU-hibrid szuperszámítógép 
A grafikus chipek eddigi talán legnagyobb publikus sikerének a Folding@home elosztott fehérjekutatás számít, ahol a számítási teljesítmény több mint felét NVIDIA és AMD/ATI grafikus chipek adják, míg szakmai áttörést a tokiói TSUBAME szuperszámítógép frissítése ért el. A TSUBAME Tesla 10P rackelhetõ rendszerekben összesen 680 darab NVIDIA GT200 chippel bõvült, melyekkel 52 teraflopsnyi számítási kapacitás érkezett mindössze 136 kilowattos maximális energiafogyasztás mellett. A lépést nagyrészt a memóriasávszélesség igénye indokolta, a bõvítés másodpercenkénti 69 360 gigabájtos addicionális áteresztõképességet adott az installációhoz, amit 2700 darab kétutas Xeon HPC-rendszerrel egyenértékû.
Mindez azonban messze nem jelenti azt, hogy a számítási erõforrásokban dúskáló grafikus chipek megváltották volna a világot, és minden problémára jó választ adnának. Éppen ezért érdemes lesz résen lenni a jövõben is az ilyen és ehhez hasonló állításoknál, mint amilyen a SETI@home kapcsán született.